[["0",{"pageContent":"LangChain Cookbook ‍‍\nThis cookbook is based off the LangChain Conceptual Documentation\nGoal: Provide an introductory understanding of the components and use cases of \nLangChain via ELI5 examples and code snippets. For use cases check out part 2 (coming \nsoon).\nLinks:\n•LC Conceptual Documentation\n•LC Python Documentation\n•LC Javascript/Typescript Documentation\n•LC Discord\n•www.langchain.com\n•LC Twitter\nWhat is LangChain?\nLangChain is a framework for developing applications powered by language \nmodels.\nTLDR: LangChain makes the complicated parts of working & building with AI models \neasier. It helps do this in two ways:\n1.Integration - Bring external data, such as your files, other applications, and api data,\nto your LLMs\n2.Agency - Allow your LLMs to interact with it's environment via decision making. \nUse LLMs to help decide which action to take next\nWhy LangChain?\n1.Components - LangChain makes it easy to swap out abstractions and components \nnecessary to work with language models.","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":1,"to":24}}}}],["1",{"pageContent":"Use LLMs to help decide which action to take next\nWhy LangChain?\n1.Components - LangChain makes it easy to swap out abstractions and components \nnecessary to work with language models.\n2.Customized Chains - LangChain provides out of the box support for using and \ncustomizing 'chains' - a series of actions strung together.\n3.Speed  - This team ships insanely fast. You'll be up to date with the latest LLM \nfeatures.\n4.Community  - Wonderful discord and community support, meet ups, hackathons, \netc.\nThough LLMs can be straightforward (text-in, text-out) you'll quickly run into friction \npoints that LangChain helps with once you develop more complicated applications.","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":24,"to":35}}}}],["2",{"pageContent":"Note: This cookbook will not cover all aspects of LangChain. It's contents have been curated to\nget you to building & impact as quick as possible. For more, please check out LangChain \nConceptual Documentation\nopenai_api_key='YourAPIKey'\nLangChain Components\nSchema - Nuts and Bolts of working with LLMs\nText\nThe natural language way to interact with LLMs\n# You'll be working with simple strings (that'll soon grow in \ncomplexity!)\nmy_text = \"What day comes after Friday?\"\nChat Messages\nLike text, but specified with a message type (System, Human, AI)\n•System - Helpful background context that tell the AI what to do\n•Human - Messages that are intented to represent the user\n•AI - Messages that show what the AI responded with\nFor more, see OpenAI's documentation\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import HumanMessage, SystemMessage, AIMessage\nchat = ChatOpenAI(temperature=.7, openai_api_key=openai_api_key)\nchat(\n    [","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":37,"to":58}}}}],["3",{"pageContent":"from langchain.chat_models import ChatOpenAI\nfrom langchain.schema import HumanMessage, SystemMessage, AIMessage\nchat = ChatOpenAI(temperature=.7, openai_api_key=openai_api_key)\nchat(\n    [\n        SystemMessage(content=\"You are a nice AI bot that helps a user\nfigure out what to eat in one short sentence\"),\n        HumanMessage(content=\"I like tomatoes, what should I eat?\")\n    ]\n)\nAIMessage(content='You could try making a tomato salad with fresh \nbasil and mozzarella cheese.', additional_kwargs={})\nYou can also pass more chat history w/ responses from the AI\nchat(\n    [\n        SystemMessage(content=\"You are a nice AI bot that helps a user\nfigure out where to travel in one short sentence\"),","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":58,"to":74}}}}],["4",{"pageContent":"HumanMessage(content=\"I like the beaches where should I go?\"),\n        AIMessage(content=\"You should go to Nice, France\"),\n        HumanMessage(content=\"What else should I do when I'm there?\")\n    ]\n)\nAIMessage(content='While in Nice, you can also explore the charming \nold town, visit the famous Promenade des Anglais, and indulge in \ndelicious French cuisine.', additional_kwargs={})\nDocuments\nAn object that holds a piece of text and metadata (more information about that text)\nfrom langchain.schema import Document\nDocument(page_content=\"This is my document. It is full of text that \nI've gathered from other places\",\n         metadata={\n             'my_document_id' : 234234,\n             'my_document_source' : \"The LangChain Papers\",\n             'my_document_create_time' : 1680013019\n         })\nDocument(page_content=\"This is my document. It is full of text that \nI've gathered from other places\", lookup_str='', \nmetadata={'my_document_id': 234234, 'my_document_source': 'The","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":76,"to":96}}}}],["5",{"pageContent":"})\nDocument(page_content=\"This is my document. It is full of text that \nI've gathered from other places\", lookup_str='', \nmetadata={'my_document_id': 234234, 'my_document_source': 'The \nLangChain Papers', 'my_document_create_time': 1680013019}, \nlookup_index=0)\nModels - The interface to the AI brains\nLanguage Model\nA model that does text in     text out!➡‍\nCheck out how I changed the model I was using from the default one to ada-001. See more \nmodels here\nfrom langchain.llms import OpenAI\nllm = OpenAI(model_name=\"text-ada-001\", openai_api_key=openai_api_key)\nllm(\"What day comes after Friday?\")\n'\\n\\nSaturday.'\nChat Model\nA model that takes a series of messages and returns a message output\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import HumanMessage, SystemMessage, AIMessage","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":96,"to":114}}}}],["6",{"pageContent":"chat = ChatOpenAI(temperature=1, openai_api_key=openai_api_key)\nchat(\n    [\n        SystemMessage(content=\"You are an unhelpful AI bot that makes \na joke at whatever the user says\"),\n        HumanMessage(content=\"I would like to go to New York, how \nshould I do this?\")\n    ]\n)\nAIMessage(content=\"You could try walking, but I don't recommend it \nunless you have a lot of time on your hands. Maybe try flapping your \narms really hard and see if you can fly there?\", additional_kwargs={})\nText Embedding Model\nChange your text into a vector (a series of numbers that hold the semantic 'meaning' of \nyour text). Mainly used when comparing two pieces of text together.\nBTW: Semantic means 'relating to meaning in language or logic.'\nfrom langchain.embeddings import OpenAIEmbeddings\nembeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\ntext = \"Hi! It's time for the beach\"\ntext_embedding = embeddings.embed_query(text)\nprint (f\"Your embedding is length {len(text_embedding)}\")","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":116,"to":136}}}}],["7",{"pageContent":"embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\ntext = \"Hi! It's time for the beach\"\ntext_embedding = embeddings.embed_query(text)\nprint (f\"Your embedding is length {len(text_embedding)}\")\nprint (f\"Here's a sample: {text_embedding[:5]}...\")\nYour embedding is length 1536\nHere's a sample: [-0.00020583387231454253, -0.003205398330464959, -\n0.0008301587076857686, -0.01946892775595188, -0.015162716619670391]...\nPrompts - Text generally used as instructions to your model\nPrompt\nWhat you'll pass to the underlying model\nfrom langchain.llms import OpenAI\nllm = OpenAI(model_name=\"text-davinci-003\", \nopenai_api_key=openai_api_key)\n# I like to use three double quotation marks for my prompts because \nit's easier to read\nprompt = \"\"\"\nToday is Monday, tomorrow is Wednesday.","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":136,"to":153}}}}],["8",{"pageContent":"What is wrong with that statement?\n\"\"\"\nllm(prompt)\n'\\nThe statement is incorrect; tomorrow is Tuesday, not Wednesday.'\nPrompt Template\nAn object that helps create prompts based on a combination of user input, other non-static \ninformation and a fixed template string.\nThink of it as an f-string in python but for prompts\nfrom langchain.llms import OpenAI\nfrom langchain import PromptTemplate\nllm = OpenAI(model_name=\"text-davinci-003\", \nopenai_api_key=openai_api_key)\n# Notice \"location\" below, that is a placeholder for another value \nlater\ntemplate = \"\"\"\nI really want to travel to {location}. What should I do there?\nRespond in one short sentence\n\"\"\"\nprompt = PromptTemplate(\n    input_variables=[\"location\"],\n    template=template,\n)\nfinal_prompt = prompt.format(location='Rome')\nprint (f\"Final Prompt: {final_prompt}\")\nprint (\"-----------\")\nprint (f\"LLM Output: {llm(final_prompt)}\")\nFinal Prompt: \nI really want to travel to Rome. What should I do there?\nRespond in one short sentence\n-----------\nLLM Output:","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":155,"to":185}}}}],["9",{"pageContent":"print (\"-----------\")\nprint (f\"LLM Output: {llm(final_prompt)}\")\nFinal Prompt: \nI really want to travel to Rome. What should I do there?\nRespond in one short sentence\n-----------\nLLM Output: \nVisit the Colosseum, the Pantheon, and the Trevi Fountain for a taste \nof Rome's ancient and modern culture.","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":185,"to":193}}}}],["10",{"pageContent":"Example Selectors\nAn easy way to select from a series of examples that allow you to dynamic place in-context \ninformation into your prompt. Often used when your task is nuanced or you have a large \nlist of examples.\nCheck out different types of example selectors here\nIf you want an overview on why examples are important (prompt engineering), check out \nthis video\nfrom langchain.prompts.example_selector import \nSemanticSimilarityExampleSelector\nfrom langchain.vectorstores import FAISS\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.prompts import FewShotPromptTemplate, PromptTemplate\nfrom langchain.llms import OpenAI\nllm = OpenAI(model_name=\"text-davinci-003\", \nopenai_api_key=openai_api_key)\nexample_prompt = PromptTemplate(\n    input_variables=[\"input\", \"output\"],\n    template=\"Example Input: {input}\\nExample Output: {output}\",\n)\n# Examples of locations that nouns are found\nexamples = [\n    {\"input\": \"pirate\", \"output\": \"ship\"},\n    {\"input\": \"pilot\", \"output\": \"plane\"},","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":195,"to":217}}}}],["11",{"pageContent":")\n# Examples of locations that nouns are found\nexamples = [\n    {\"input\": \"pirate\", \"output\": \"ship\"},\n    {\"input\": \"pilot\", \"output\": \"plane\"},\n    {\"input\": \"driver\", \"output\": \"car\"},\n    {\"input\": \"tree\", \"output\": \"ground\"},\n    {\"input\": \"bird\", \"output\": \"nest\"},\n]\n# SemanticSimilarityExampleSelector will select examples that are \nsimilar to your input by semantic meaning\nexample_selector = SemanticSimilarityExampleSelector.from_examples(\n    # This is the list of examples available to select from.\n    examples, \n    \n    # This is the embedding class used to produce embeddings which are\nused to measure semantic similarity.\n    OpenAIEmbeddings(openai_api_key=openai_api_key), \n    \n    # This is the VectorStore class that is used to store the \nembeddings and do a similarity search over.\n    FAISS,","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":217,"to":238}}}}],["12",{"pageContent":"# This is the number of examples to produce.\n    k=2\n)\nsimilar_prompt = FewShotPromptTemplate(\n    # The object that will help select examples\n    example_selector=example_selector,\n    \n    # Your prompt\n    example_prompt=example_prompt,\n    \n    # Customizations that will be added to the top and bottom of your \nprompt\n    prefix=\"Give the location an item is usually found in\",\n    suffix=\"Input: {noun}\\nOutput:\",\n    \n    # What inputs your prompt will receive\n    input_variables=[\"noun\"],\n)\n# Select a noun!\nmy_noun = \"student\"\nprint(similar_prompt.format(noun=my_noun))\nGive the location an item is usually found in\nExample Input: driver\nExample Output: car\nExample Input: pilot\nExample Output: plane\nInput: student\nOutput:\nllm(similar_prompt.format(noun=my_noun))\n' classroom'\nOutput Parsers\nA helpful way to format the output of a model. Usually used for structured output.\nTwo big concepts:\n1. Format Instructions - A autogenerated prompt that tells the LLM how to format it's \nresponse based off your desired result","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":241,"to":275}}}}],["13",{"pageContent":"Two big concepts:\n1. Format Instructions - A autogenerated prompt that tells the LLM how to format it's \nresponse based off your desired result\n2. Parser - A method which will extract your model's text output into a desired structure \n(usually json)","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":275,"to":279}}}}],["14",{"pageContent":"from langchain.output_parsers import StructuredOutputParser, \nResponseSchema\nfrom langchain.prompts import ChatPromptTemplate, \nHumanMessagePromptTemplate\nfrom langchain.llms import OpenAI\nllm = OpenAI(model_name=\"text-davinci-003\", \nopenai_api_key=openai_api_key)\n# How you would like your reponse structured. This is basically a \nfancy prompt template\nresponse_schemas = [\n    ResponseSchema(name=\"bad_string\", description=\"This a poorly \nformatted user input string\"),\n    ResponseSchema(name=\"good_string\", description=\"This is your \nresponse, a reformatted response\")\n]\n# How you would like to parse your output\noutput_parser = \nStructuredOutputParser.from_response_schemas(response_schemas)\n# See the prompt template you created for formatting\nformat_instructions = output_parser.get_format_instructions()\nprint (format_instructions)\nThe output should be a markdown code snippet formatted in the \nfollowing schema:\n```json\n{\n\"bad_string\": string  // This a poorly formatted user input \nstring","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":281,"to":307}}}}],["15",{"pageContent":"print (format_instructions)\nThe output should be a markdown code snippet formatted in the \nfollowing schema:\n```json\n{\n\"bad_string\": string  // This a poorly formatted user input \nstring\n\"good_string\": string  // This is your response, a reformatted \nresponse\n}\n```\ntemplate = \"\"\"\nYou will be given a poorly formatted string from a user.\nReformat it and make sure all the words are spelled correctly\n{format_instructions}\n% USER INPUT:\n{user_input}\nYOUR RESPONSE:\n\"\"\"\nprompt = PromptTemplate(","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":307,"to":326}}}}],["16",{"pageContent":"input_variables=[\"user_input\"],\n    partial_variables={\"format_instructions\": format_instructions},\n    template=template\n)\npromptValue = prompt.format(user_input=\"welcom to califonya!\")\nprint(promptValue)\nYou will be given a poorly formatted string from a user.\nReformat it and make sure all the words are spelled correctly\nThe output should be a markdown code snippet formatted in the \nfollowing schema:\n```json\n{\n\"bad_string\": string  // This a poorly formatted user input \nstring\n\"good_string\": string  // This is your response, a reformatted \nresponse\n}\n```\n% USER INPUT:\nwelcom to califonya!\nYOUR RESPONSE:\nllm_output = llm(promptValue)\nllm_output\n'```json\\n{\\n\\t\"bad_string\": \"welcom to califonya!\",\\n\\t\"good_string\":\n\"Welcome to California!\"\\n}\\n```'\noutput_parser.parse(llm_output)\n{'bad_string': 'welcom to califonya!', 'good_string': 'Welcome to \nCalifornia!'}\nIndexes - Structuring documents to LLMs can work with them\nDocument Loaders","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":328,"to":357}}}}],["17",{"pageContent":"output_parser.parse(llm_output)\n{'bad_string': 'welcom to califonya!', 'good_string': 'Welcome to \nCalifornia!'}\nIndexes - Structuring documents to LLMs can work with them\nDocument Loaders\nEasy ways to import data from other sources. Shared functionality with OpenAI Plugins \nspecifically retrieval plugins\nSee a big list of document loaders here. A bunch more on Llama Index as well.","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":357,"to":364}}}}],["18",{"pageContent":"from langchain.document_loaders import HNLoader\nloader = HNLoader(\"https://news.ycombinator.com/item?id=34422627\")\ndata = loader.load()\nprint (f\"Found {len(data)} comments\")\nprint (f\"Here's a sample:\\n\\n{''.join([x.page_content[:150] for x in \ndata[:2]])}\")\nFound 76 comments\nHere's a sample:\ndang 69 days ago  \n             | next [–] \nRelated ongoing thread:GPT-3.5 and Wolfram Alpha via LangChain - \nhttps://news.ycombinator.com/item?id=344Ozzie_osman 69 days ago  \n             | prev | next [–] \nLangChain is awesome. For people not sure what it's doing, large \nlanguage models (LLMs) are\nText Splitters\nOften times your document is too long (like a book) for your LLM. You need to split it up \ninto chunks. Text splitters help with this.\nThere are many ways you could split your text into chunks, experiment with different ones \nto see which is best for you.\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n# This is a long document we can split up.","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":366,"to":387}}}}],["19",{"pageContent":"to see which is best for you.\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n# This is a long document we can split up.\nwith open('data/PaulGrahamEssays/worked.txt') as f:\n    pg_work = f.read()\n    \nprint (f\"You have {len([pg_work])} document\")\nYou have 1 document\ntext_splitter = RecursiveCharacterTextSplitter(\n    # Set a really small chunk size, just to show.\n    chunk_size = 150,\n    chunk_overlap  = 20,\n)\ntexts = text_splitter.create_documents([pg_work])\nprint (f\"You have {len(texts)} documents\")\nYou have 606 documents","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":387,"to":402}}}}],["20",{"pageContent":"print (\"Preview:\")\nprint (texts[0].page_content, \"\\n\")\nprint (texts[1].page_content)\nPreview:\nFebruary 2021Before college the two main things I worked on, outside \nof school,\nwere writing and programming. I didn't write essays. I wrote what \nbeginning writers were supposed to write then, and probably still\nare: short stories. My stories were awful. They had hardly any plot,\nRetrievers\nEasy way to combine documents with language models.\nThere are many different types of retrievers, the most widely supported is the \nVectoreStoreRetriever\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import FAISS\nfrom langchain.embeddings import OpenAIEmbeddings\nloader = TextLoader('data/PaulGrahamEssays/worked.txt')\ndocuments = loader.load()\n# Get your splitter ready\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, \nchunk_overlap=50)\n# Split your docs into texts\ntexts = text_splitter.split_documents(documents)","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":404,"to":427}}}}],["21",{"pageContent":"# Get your splitter ready\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, \nchunk_overlap=50)\n# Split your docs into texts\ntexts = text_splitter.split_documents(documents)\n# Get embedding engine ready\nembeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n# Embedd your texts\ndb = FAISS.from_documents(texts, embeddings)\n# Init your retriever. Asking for just 1 document back\nretriever = db.as_retriever()\nretriever\nVectorStoreRetriever(vectorstore=<langchain.vectorstores.faiss.FAISS \nobject at 0x7fb81007a9d0>, search_type='similarity', search_kwargs={})\ndocs = retriever.get_relevant_documents(\"what types of things did the \nauthor want to build?\")\nprint(\"\\n\\n\".join([x.page_content[:200] for x in docs[:2]]))","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":427,"to":443}}}}],["22",{"pageContent":"standards; what was the point? No one else wanted one either, so\noff they went. That was what happened to systems work.I wanted not \njust to build things, but to build things that would\nlast.In this di\nmuch of it in grad school.Computer Science is an uneasy alliance \nbetween two halves, theory\nand systems. The theory people prove things, and the systems people\nbuild things. I wanted to build things. \nVectorStores\nDatabases to store vectors. Most popular ones are Pinecone & Weaviate. More examples on \nOpenAIs retriever documentation. Chroma & FAISS are easy to work with locally.\nConceptually, think of them as tables w/ a column for embeddings (vectors) and a column \nfor metadata.\nExample\nEmbeddingMetadata\n[-0.00015641732898075134, -\n0.003165106289088726, ...]\n{'date' : '1/2/23}\n[-0.00035465431654651654, \n1.4654131651654516546, ...]\n{'date' : '1/3/23}\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import FAISS","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":445,"to":468}}}}],["23",{"pageContent":"1.4654131651654516546, ...]\n{'date' : '1/3/23}\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import FAISS\nfrom langchain.embeddings import OpenAIEmbeddings\nloader = TextLoader('data/PaulGrahamEssays/worked.txt')\ndocuments = loader.load()\n# Get your splitter ready\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, \nchunk_overlap=50)\n# Split your docs into texts\ntexts = text_splitter.split_documents(documents)\n# Get embedding engine ready\nembeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\nprint (f\"You have {len(texts)} documents\")\nYou have 78 documents\nembedding_list = embeddings.embed_documents([text.page_content for \ntext in texts])","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":468,"to":486}}}}],["24",{"pageContent":"print (f\"You have {len(embedding_list)} embeddings\")\nprint (f\"Here's a sample of one: {embedding_list[0][:3]}...\")\nYou have 78 embeddings\nHere's a sample of one: [-0.0011257503647357225, -0.01111479103565216,\n-0.012860921211540699]...\nYour vectorstore store your embeddings (   ) and make the easily searchable☝‍\nMemory\nHelping LLMs remember information.\nMemory is a bit of a loose term. It could be as simple as remembering information you've \nchatted about in the past or more complicated information retrieval.\nWe'll keep it towards the Chat Message use case. This would be used for chat bots.\nThere are many types of memory, explore the documentation to see which one fits your use\ncase.\nChat Message History\nfrom langchain.memory import ChatMessageHistory\nfrom langchain.chat_models import ChatOpenAI\nchat = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\nhistory = ChatMessageHistory()\nhistory.add_ai_message(\"hi!\")\nhistory.add_user_message(\"what is the capital of france?\")\nhistory.messages","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":488,"to":508}}}}],["25",{"pageContent":"chat = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\nhistory = ChatMessageHistory()\nhistory.add_ai_message(\"hi!\")\nhistory.add_user_message(\"what is the capital of france?\")\nhistory.messages\n[AIMessage(content='hi!', additional_kwargs={}),\n HumanMessage(content='what is the capital of france?', \nadditional_kwargs={})]\nai_response = chat(history.messages)\nai_response\nAIMessage(content='The capital of France is Paris.', \nadditional_kwargs={})\nhistory.add_ai_message(ai_response.content)\nhistory.messages\n[AIMessage(content='hi!', additional_kwargs={}),\n HumanMessage(content='what is the capital of france?', \nadditional_kwargs={}),","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":508,"to":524}}}}],["26",{"pageContent":"AIMessage(content='The capital of France is Paris.', \nadditional_kwargs={})]\nChains ⛓‍⛓‍⛓‍\nCombining different LLM calls and action automatically\nEx: Summary #1, Summary #2, Summary #3 > Final Summary\nCheck out this video explaining different summarization chain types\nThere are many applications of chains search to see which are best for your use case.\nWe'll cover two of them:\n1. Simple Sequential Chains\nEasy chains where you can use the output of an LMM as an input into another. Good for \nbreaking up tasks (and keeping your LLM focused)\nfrom langchain.llms import OpenAI\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import SimpleSequentialChain\nllm = OpenAI(temperature=1, openai_api_key=openai_api_key)\ntemplate = \"\"\"Your job is to come up with a classic dish from the area\nthat the users suggests.\n% USER LOCATION\n{user_location}\nYOUR RESPONSE:\n\"\"\"\nprompt_template = PromptTemplate(input_variables=[\"user_location\"], \ntemplate=template)","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":526,"to":549}}}}],["27",{"pageContent":"that the users suggests.\n% USER LOCATION\n{user_location}\nYOUR RESPONSE:\n\"\"\"\nprompt_template = PromptTemplate(input_variables=[\"user_location\"], \ntemplate=template)\n# Holds my 'location' chain\nlocation_chain = LLMChain(llm=llm, prompt=prompt_template)\ntemplate = \"\"\"Given a meal, give a short and simple recipe on how to \nmake that dish at home.\n% MEAL\n{user_meal}\nYOUR RESPONSE:\n\"\"\"\nprompt_template = PromptTemplate(input_variables=[\"user_meal\"], \ntemplate=template)","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":549,"to":565}}}}],["28",{"pageContent":"# Holds my 'meal' chain\nmeal_chain = LLMChain(llm=llm, prompt=prompt_template)\noverall_chain = SimpleSequentialChain(chains=[location_chain, \nmeal_chain], verbose=True)\nreview = overall_chain.run(\"Rome\")\n> Entering new SimpleSequentialChain chain...\nA classic dish from Rome is Spaghetti alla Carbonara, a pasta dish \nmade with egg, cheese, guanciale (cured pork cheek), and black pepper.\nIngredients: \n-1/2 lb. spaghetti\n-4 oz. guanciale, diced\n-2 cloves garlic, minced\n-2 eggs\n-2/3 cup Parmigiano Reggiano cheese, divided\n-1/4 tsp. freshly cracked black pepper\n-1/4 cup reserved pasta water\n-2 tablespoons olive oil\n-Parsley for garnish (optional)\nInstructions:\n1. Boil spaghetti in a large pot of salted boiling water until al \ndente, about 8 minutes. Reserve 1/4 cup of cooking water and drain the\nspaghetti.\n2. In a large skillet, heat oil over medium-high heat, then add \nguanciale and sauté until lightly brown, about 5 minutes.\n3. Add garlic and sauté for an additional 1-2 minutes.","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":567,"to":591}}}}],["29",{"pageContent":"spaghetti.\n2. In a large skillet, heat oil over medium-high heat, then add \nguanciale and sauté until lightly brown, about 5 minutes.\n3. Add garlic and sauté for an additional 1-2 minutes.\n4. In a medium bowl, whisk together eggs and 1/3 cup Parmigiano \nReggiano cheese.\n5. Add cooked spaghetti to the large skillet, toss to combine, then \nreduce the heat to medium-low.\n6. Pour in the egg and cheese mixture, then add pepper and reserved \npasta water.\n7. Toss pasta\n> Finished chain.","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":591,"to":602}}}}],["30",{"pageContent":"2. Summarization Chain\nEasily run through long numerous documents and get a summary. Check out this video for \nother chain types besides map-reduce\nfrom langchain.chains.summarize import load_summarize_chain\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nloader = TextLoader('data/PaulGrahamEssays/disc.txt')\ndocuments = loader.load()\n# Get your splitter ready\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=700, \nchunk_overlap=50)\n# Split your docs into texts\ntexts = text_splitter.split_documents(documents)\n# There is a lot of complexity hidden in this one line. I encourage \nyou to check out the video above for more detail\nchain = load_summarize_chain(llm, chain_type=\"map_reduce\", \nverbose=True)\nchain.run(texts)\n> Entering new MapReduceDocumentsChain chain...\nPrompt after formatting:\nWrite a concise summary of the following:\n\"January 2017Because biographies of famous scientists tend to","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":604,"to":625}}}}],["31",{"pageContent":"chain.run(texts)\n> Entering new MapReduceDocumentsChain chain...\nPrompt after formatting:\nWrite a concise summary of the following:\n\"January 2017Because biographies of famous scientists tend to \nedit out their mistakes, we underestimate the \ndegree of risk they were willing to take.\nAnd because anything a famous scientist did that\nwasn't a mistake has probably now become the\nconventional wisdom, those choices don't\nseem risky either.Biographies of Newton, for example, understandably \nfocus\nmore on physics than alchemy or theology.\nThe impression we get is that his unerring judgment\nled him straight to truths no one else had noticed.\nHow to explain all the time he spent on alchemy\nand theology?  Well, smart people are often kind of\ncrazy.But maybe there is a simpler explanation. Maybe\"\nCONCISE SUMMARY:\nPrompt after formatting:","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":625,"to":644}}}}],["32",{"pageContent":"Write a concise summary of the following:\n\"the smartness and the craziness were not as separate\nas we think. Physics seems to us a promising thing\nto work on, and alchemy and theology obvious wastes\nof time. But that's because we know how things\nturned out. In Newton's day the three problems \nseemed roughly equally promising. No one knew yet\nwhat the payoff would be for inventing what we\nnow call physics; if they had, more people would \nhave been working on it. And alchemy and theology\nwere still then in the category Marc Andreessen would \ndescribe as \"huge, if true.\"Newton made three bets. One of them \nworked. But \nthey were all risky.\"\nCONCISE SUMMARY:\n> Entering new LLMChain chain...\nPrompt after formatting:\nWrite a concise summary of the following:\n\" Biographies of famous scientists often omit the risks they took and \nthe mistakes they made during their lifetime. This gives us an \nimpression that these scientists had a perfect judgement, when in fact","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":646,"to":666}}}}],["33",{"pageContent":"the mistakes they made during their lifetime. This gives us an \nimpression that these scientists had a perfect judgement, when in fact\nthey made unwise decisions like Newton's dabblings in alchemy and \ntheology. Perhaps these scientists were just taking risks and making \nmistakes like anyone else.\nThis passage discusses how, in the time of Sir Isaac Newton, the three\nareas of study – physics, alchemy, and theology – were all considered \nequally valuable and worthy of exploration. Newton's success in the \narea of physics has since made the others seem like a waste of time, \nhowever at the point of Newton's exploration, all three were seen as \nhigh-risk but high-reward propositions.\"\nCONCISE SUMMARY:\n> Finished chain.\n> Finished chain.","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":666,"to":679}}}}],["34",{"pageContent":"\" Biographies of famous scientists often omit the risks they took and \nmistakes they made, creating an impression of perfect judgement. Sir \nIsaac Newton's exploration of physics, alchemy, and theology was seen \nas all high-risk but high-reward propositions at the time, and should \nnot be overlooked.\"\nAgents 烙烙\nOfficial LangChain Documentation describes agents perfectly (emphasis mine):\nSome applications will require not just a predetermined chain of calls to \nLLMs/other tools, but potentially an unknown chain that depends on the user's \ninput. In these types of chains, there is a “agent” which has access to a suite of \ntools. Depending on the user input, the agent can then decide which, if any, of \nthese tools to call.\nBasically you use the LLM not just for text output, but also for decision making. The \ncoolness and power of this functionality can't be overstated enough.\nSam Altman emphasizes that the LLMs are good 'reasoning engine'. Agent take advantage \nof this.\nAgents","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":681,"to":697}}}}],["35",{"pageContent":"coolness and power of this functionality can't be overstated enough.\nSam Altman emphasizes that the LLMs are good 'reasoning engine'. Agent take advantage \nof this.\nAgents\nThe language model that drives decision making.\nMore specifically, an agent takes in an input and returns a response corresponding to an \naction to take along with an action input. You can see different types of agents (which are \nbetter for different use cases) here.\nTools\nA 'capability' of an agent. This is an abstraction on top of a function that makes it easy for \nLLMs (and agents) to interact with it. Ex: Google search.\nThis area shares commonalities with OpenAI plugins.\nToolkit\nGroups of tools that your agent can select from\nLet's bring them all together:\nfrom langchain.agents import load_tools\nfrom langchain.agents import initialize_agent\nfrom langchain.llms import OpenAI\nimport json\nllm = OpenAI(temperature=0, openai_api_key=openai_api_key)\nserpapi_api_key='...'","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":697,"to":717}}}}],["36",{"pageContent":"toolkit = load_tools([\"serpapi\"], llm=llm, \nserpapi_api_key=serpapi_api_key)\nagent = initialize_agent(toolkit, llm, agent=\"zero-shot-react-\ndescription\", verbose=True, return_intermediate_steps=True)\nresponse = agent({\"input\":\"what was the first album of the\" \n                    \"band that Natalie Bergman is a part of?\"})\n> Entering new AgentExecutor chain...\n I should try to find out what band Natalie Bergman is a part of.\nAction: Search\nAction Input: \"Natalie Bergman band\"\nObservation: Natalie Bergman is an American singer-songwriter. She is \none half of the duo Wild Belle, along with her brother Elliot Bergman.\nHer debut solo album, Mercy, was released on Third Man Records on May \n7, 2021. She is based in Los Angeles.\nThought: I should search for the debut album of Wild Belle.\nAction: Search\nAction Input: \"Wild Belle debut album\"\nObservation: Isles\nThought: I now know the final answer.\nFinal Answer: Isles is the debut album of Wild Belle, the band that \nNatalie Bergman is a part of.\n> Finished chain.","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":719,"to":740}}}}],["37",{"pageContent":"Observation: Isles\nThought: I now know the final answer.\nFinal Answer: Isles is the debut album of Wild Belle, the band that \nNatalie Bergman is a part of.\n> Finished chain.\nprint(json.dumps(response[\"intermediate_steps\"], indent=2))\n[\n  [\n    [\n      \"Search\",\n      \"Natalie Bergman band\",\n      \" I should try to find out what band Natalie Bergman is a part \nof.\\nAction: Search\\nAction Input: \\\"Natalie Bergman band\\\"\"\n    ],\n    \"Natalie Bergman is an American singer-songwriter. She is one half\nof the duo Wild Belle, along with her brother Elliot Bergman. Her \ndebut solo album, Mercy, was released on Third Man Records on May 7, \n2021. She is based in Los Angeles.\"\n  ],\n  [\n    [\n      \"Search\",\n      \"Wild Belle debut album\",\n      \" I should search for the debut album of Wild Belle.\\nAction: \nSearch\\nAction Input: \\\"Wild Belle debut album\\\"\"\n    ],","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":740,"to":765}}}}],["38",{"pageContent":"\"Isles\"\n  ]\n]\nWild Belle\nEnjoy    https://open.spotify.com/track/1eREJIBdqeCcqNCB1pbz7w?\nsi=c014293b63c7478c","metadata":{"source":"blob","blobType":"application/pdf","pdf_numpages":20,"loc":{"lines":{"from":767,"to":772}}}}]]